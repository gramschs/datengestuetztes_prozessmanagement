{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b18ce5-5d01-40d0-8723-d7ce245bf6e3",
   "metadata": {},
   "source": [
    "# Neuronale Netze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edfec85-6ce9-438a-a3eb-eed0fe8861a9",
   "metadata": {},
   "source": [
    "Durch neuronale Netze, die tief verschachtelt sind (= tiefe neuronale Netze = deep neural network), gab es im Bereich des maschinellen Lernens einen Durchbruch. Neuronale Netze sind eine Technik aus der Statistik, die bereits in den 1940er Jahren entwickelt wurde. Seit ca. 10 Jahren erleben diese Techniken verbunden mit Fortschritten in der Computertechnologie eine Renaissance.\n",
    "\n",
    "Neuronale Netze bzw. Deep Learning kommt vor allem da zum Einsatz, wo es kaum systematisches Wissen gibt. Damit neuronale Netze erfolgreich trainiert werden können, brauchen sie sehr große Datenmengen. Nachdem in den letzten 15 Jahren mit dem Aufkommen von Smartphones die Daten im Bereich Videos und Fotos massiv zugenommen haben, lohnt sich der Einsatz der neuronalen Netze fúr Spracherkennung, Gesichtserkennung oder Texterkennung besonders.\n",
    "\n",
    "Beispielsweise hat ein junges deutsches Start-Up-Unternehmen 2017 aus einem neuronalen Netz zum Übersetzen Englisch <-> Deutsch eine Webanwendung entwickelt und ins Internet gestellt, die meinen Alltag massiv beeinflusst: DeepL.com, siehe\n",
    "> https://www.deepl.com/en/blog/how-does-deepl-work\n",
    "\n",
    "Mittlerweile beherrscht DeepL 23 Sprachen, siehe auch den Wikipedia-Artikel zu DeepL:\n",
    "> https://de.wikipedia.org/wiki/DeepL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e916770-8c5d-40d5-9bd6-27d219087194",
   "metadata": {},
   "source": [
    "## Ein einzelnes künstliches Neuron - das Perzeptron \n",
    "\n",
    "1943 haben die Forscher Warren McCulloch und Walter Pitts das erste Modell einer vereinfachten Hirnzelle präsentiert. Zu Ehren der beiden Forscher heißt dieses Modell MPC-Neuron. Darauf aufbauend publizierte Frank Rosenblatt 1957 seine Idee einer Lernregel für das künstliche Neuron. Das sogenannte Perzeptron bildet bis heute die Grundlage der künstlichen neuronalen Netze. Inspiriert wurden die Forscher dabei durch den Aufbau des Gehirns und der Verknüpfung der Nervenzellen.\n",
    "\n",
    "![Neuron](pics/neuron_wikipedia.png \"Neuron\")\n",
    "\n",
    "Schematische Darstellung einer Nervenzelle - CC BY-SA 3.0 \n",
    "(Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Künstliches_Neuron#/media/Datei:Neuron_(deutsch)-1.svg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1aa293-e234-4479-9827-37aad1edc5f2",
   "metadata": {},
   "source": [
    "Elektrische und chemische Eingabesignale kommen bei den Dendriten an und laufen im Zellkörper zusammen. Sobald ein bestimmter Schwellwert überschritten wird, wird ein Ausgabesignal erzeugt und über das Axon weitergeleitet. Mehr Details zu Nervenzellen finden Sie bei Wikipedia: https://de.wikipedia.org/wiki/Nervenzelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3a485-cf0a-434c-8c44-4f5bb9625e88",
   "metadata": {},
   "source": [
    "### Definition eines künstlichen Neurons\n",
    "\n",
    "Das einfachste künstliche Neuron besteht aus zwei Inputs und einem Output. Dabei sind für die beiden Inputs nur zwei Zustände zugelassen und auch der Output besteht nur aus zwei verschiedenen Zuständen. In der Sprache des maschinellen Lernens liegt also eine binäre Klassifikationsaufgabe innerhalb des Supervised Learnings vor.\n",
    "\n",
    "Beispiel:\n",
    "* Input 1: Es regnet oder es regnet nicht.\n",
    "* Input 2: Der Rasensprenger ist an oder nicht.\n",
    "* Output: Der Rasen wird nass oder nicht.\n",
    "\n",
    "Den Zusammenhang zwischen Regen, Rasensprenger und nassem Rasen können wir in einer Tabelle abbilden:\n",
    "\n",
    "Regnet es? | Ist Sprenger an? | Wird Rasen nass?\n",
    "-----------|------------------|-----------------\n",
    "nein       | nein             | nein\n",
    "ja         | nein             | ja\n",
    "nein       | ja               | ja\n",
    "ja         | ja               | ja\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a64ec-77a6-406c-837f-15be1857c71e",
   "metadata": {},
   "source": [
    "**Mini-Übung**\n",
    "\n",
    "Führen Sie zwei Variablen x1 und x2 ein und lassen Sie je nach Wert der Variablen ausgeben, ob der Rasen nass wird oder nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279d460-8d30-4265-a46e-5e0305e480b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Ihr Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b614de-58c8-4454-924d-35dcbf6b15b0",
   "metadata": {},
   "source": [
    "Für das maschinelle Lernen müssen die Daten als Zahlen aufbereitet werde, damit die maschinellen Lernverfahren in der Lage sind, Muster in den Daten zu erlernen. \"Anstatt Regnet es? Nein.\" oder Variablen mit True/False setzen wir jetzt Zahlen ein. Die Inputklassen kürzen wir mit X1 für Regen und X2 für Sprenger ab. Die 1 steht für ja, die 0 für nein. Den Output bezeichnen wir mit y.\n",
    "\n",
    "X1 | X2 | y\n",
    "---|----|---\n",
    "0  | 0  | 0\n",
    "1  | 0  | 1\n",
    "0  | 1  | 1\n",
    "1  | 1  | 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b131791-c548-4b18-ad24-ddcfca0e432f",
   "metadata": {},
   "source": [
    "Als nächstes ersetzen wir das logische ODER durch ein mathematisches Konstrukt:\n",
    "Wenn die Ungleichung\n",
    "\n",
    "$$\\omega_1 X_1 + \\omega_2 X_2 \\geq \\theta$$\n",
    "\n",
    "erfüllt ist, dann ist $y = 1$, der Rasen wird nass. Und ansonsten ist $y = 0$, der Rasen wird nicht nass. Allerdings müssen wir noch die Gewichte $\\omega_1$ und $\\omega_2$ (in Englisch: weights) geschickt wählten. Die Zahl $\\theta$ ist der griechische Buchstabe Theta und steht als Abkürzung für den sogenannten Schwellen wert (in Englisch: threshold).\n",
    "\n",
    "Beispielsweise $\\omega_1 = 0.3$, $\\omega_2=0.3$ und $\\theta = 0.2$ passen:\n",
    "* $0.3\\cdot 0 + 0.3\\cdot 0 = 0.0 \\geq 0.2$ nicht erfüllt\n",
    "* $0.3\\cdot 1 + 0.3\\cdot 0 = 0.3 \\geq 0.2$ erfüllt\n",
    "* $0.3\\cdot 0 + 0.3\\cdot 1 = 0.3 \\geq 0.2$ erfüllt\n",
    "* $0.3\\cdot 1 + 0.3\\cdot 1 = 0.6 \\geq 0.2$ erfüllt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c023cd-86ca-417b-beef-d570fde8a501",
   "metadata": {},
   "source": [
    "Noch sind wir aber nicht fertig, denn auch die Frage \"Ist die Ungleichung erfüllt oder nicht\" muss noch in eine mathematische Funktion umgeschrieben werden. Dazu subtrahieren wir auf beiden Seiten der Ungleichung den Schwellenwert $\\theta$:\n",
    "\n",
    "$$-\\theta + \\omega_1 X_1 + \\omega_2 X_2 \\geq 0.$$\n",
    "\n",
    "Damit haben wir jetzt nicht mehr einen Verglich mit dem Schwellenwert, sondern müssen nur noch entscheiden, ob der Ausdruck $-\\theta + \\omega_1 X_1 + \\omega_2 X_2$ negativ oder positiv ist. Bei negativen Werten, soll $y=0$ sein und bei positiven Werten (inklusive der Null) soll $y = 1$ sein. Dafür gibt es in der Mathematik eine passende Funktion, die sogenannte Heaviside-Funktion (manchmal auch Theta-, Stufen- oder Treppenfunktion genannt), siehe https://de.wikipedia.org/wiki/Heaviside-Funktion.\n",
    "\n",
    "![Heavisidefunktion](pics/heaviside_wikipedia.png \"Neuron\")\n",
    "\n",
    "Schaubild der Heaviside-Funktion von Lennart Kudling (Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Heaviside-Funktion#/media/Datei:Heaviside.svg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff846d-f0d4-48ef-adc9-bfd0c87bb5b5",
   "metadata": {},
   "source": [
    "Definiert ist die Heaviside-Funktion folgendermaßen:\n",
    "\n",
    "$$\\Phi(x) = \\begin{cases}0:&x<0\\\\1:&x\\geq 0\\end{cases}$$\n",
    "    \n",
    "Jetzt haben wir alles zusammen. Für gegebene Inputs $X_1$ und $X_2$ kann über\n",
    "\n",
    "$$\\Phi(-\\theta + \\omega_1X_1 + \\omega_2 X_2 )$$\n",
    "\n",
    "direkt ausgerechnet und damit klassifiziert werden, ob der Rasen nass wird oder nicht. Das Perzeptron besteht also aus einer gewichteten Summe und der Heaviside-Funktion. Das lässt sich auch für mehr Inputs verallgemeinern. Dann benutzten wir als gewichtete Summe $-\\theta + \\omega_1 X_1 + \\omega_2 X_2 + \\omega_3 X_3 + \\ldots + \\omega_N X_N = -\\theta + \\sum_{i=1}^{N} \\omega_i X_i$.\n",
    "\n",
    "Das Prinzip der gewichteten Summe ist bei allen neuronalen Netzen gleich, jedoch wird meist nicht die Heaviside-Funktion verwendet. Daher nennen wir die Funktion, die nach der gewichteten Summe angewandt wird, Aktivierungsfunktion. Also...\n",
    "\n",
    "**Ein Perzeptron ist eine gewichtete Summe von Inputs, auf die eine Aktivierungsfunktion angewandt wird. Der negative Schwellwert wird als Bias bezeichnet.**\n",
    "\n",
    "Symbolisch kann man ein Perzeptron für zwei Inputs so darstellen:\n",
    "\n",
    "![Perzeptron](pics/perzeptron.png \"Perzeptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2bd55-d27b-4c44-b35b-f90bb15daa19",
   "metadata": {},
   "source": [
    "### Wie lernt ein Perzeptron?\n",
    "\n",
    "Wir lassen einen Zufallszahlengenerator zufällig zwei Zahlen für $\\omega_1$ und $\\omega_2$ erzeugen. Betrachten wir eine Trainingsmenge mit Inputs $(X_{i,1}, X_{i,2})$ und zugehörigem $y_{i}$, so können wir sukzessive die X-Werte einsetzen und schauen, ob das zugehörige y korrekt prognostiziert wird. Wenn wir 0 herausbekommen, obwohl wir hätten 1 erwartet, so sind unsere Gewichte zu klein. Wir erhöhen die Gewichte ein wenig. Stimmt die Prognose, so lassen wir die Gewichte unverändert. Ist jedoch der prognostizierte Wert 1 anstatt 0, so verkleinern wir die Gewichte.   \n",
    "\n",
    "Klingt erst einmal nach einer guten Strategie, aber um wieviel verkleinern oder vergrößern wir die Gewichte? Eine Idee ist, die Differenz von prognostiziertem Output und richtigem Output zu nehmen, und um auf der sicheren Seite zu sein, davon beispielsweise nur 10 % oder vielleicht nur 5 %. Der Prozentsatz wird dann mit dem Buchstaben $\\alpha$ bezeichnet und **Lernrate** genannt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7576f7e-8370-404c-b756-2730fc3285aa",
   "metadata": {},
   "source": [
    "### Perzeptron mit Scikit-Learn\n",
    "\n",
    "Das Perzeptron in Scikit-Learn befindet sich im Untermodul ``sklearn.linear_model``, die Dokumentation finden Sie hier: \n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "Das Perzeptron ist nur für recht simple Klassifizierungsaufgaben geeignet. Wofür es aber gut geeignet ist, ist der Ziffern-Datensatz aus dem Untermodul ``sklearn.datasets``: \n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n",
    "\n",
    "Wir laden den Datsatz und schauen erst mal hinein. Wie bei allen in Scikit-Lern hinterlegten Datensätzen erhält man über ``.data`` die Input-Daten und ``.target`` den Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1e16b-e141-4f4b-a833-32665d0882e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print('Input X:\\n', X)\n",
    "print('Output y:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39d67a-e85c-4a2d-aa5a-7816ce37eb31",
   "metadata": {},
   "source": [
    "Insgesamt sind 1797 Daten (= Anzahl Zeilen) in ``digits`` enthalten. Die Bedeutung dessen, was im Input X enthalten ist, wird besser klar, wenn wir die 64 Spalten dazugehörigen 8x8-Pixel-Bilder darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b929e8a-fb0a-48bc-9bc4-296f36ea4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 288\n",
    "print(X[i,:])\n",
    "print(y[i])\n",
    "bild = digits.images[i]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(bild);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25bbb4-4ade-412c-9a50-72eeb0fdbee8",
   "metadata": {},
   "source": [
    "In jeder Zeile von X steckt also ein Vektor mit 64 Zahlen, die die Grauwerte eines 8x8-Bildes repräsentieren. Im entsprechenden y ist die Ziffer enthalten, die dieses Bild darstellen soll. Schauen wir mal, ob das Perzeptron aus den Grauwerten die korrekte Ziffer erlernen kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194316d5-d900-4760-9882-ff708766b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datenimport\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Auswahl des Perzeptron-Modells\n",
    "model = Perceptron()\n",
    "\n",
    "# Split in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Training\n",
    "model.fit(X_train,y_train);\n",
    "\n",
    "# Validierung\n",
    "score_train = model.score(X_train, y_train)\n",
    "score_test = model.score(X_test, y_test)\n",
    "print('Score für Trainingsdaten: {:.2f}'.format(score_train))\n",
    "print('Score für Testdaten: {:.2f}'.format(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894e350-4c42-4007-9aa9-0bf9de28ffd0",
   "metadata": {},
   "source": [
    "Funktioniert gar nicht mal schlecht :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd284c-d95b-4bb7-818f-90b1cb67cabc",
   "metadata": {},
   "source": [
    "## Neuronale Netze\n",
    "\n",
    "Nachdem anfangs die Perzeptren großen Anklang in der Forschungsgemeinde gefunden haben, wurde es ab den 1970ern still um sie. Man nennt diese Phase auch den KI-Winter (siehe https://de.wikipedia.org/wiki/KI-Winter). Einen regelrechten Durchbruch und Hype erleben Sie aber seit ca. 2010, weil\n",
    "\n",
    "1. neue Methoden entwickelt wurden, um sie zu trainieren --> Deep Learning\n",
    "2. größere Mengen von Trainingsdaten zur Verfügung stehen --> Google läßt grüßen!\n",
    "3. die Rechenpower gestiegen ist --> unglaublich, was in meinem Smartphone steckt...\n",
    "\n",
    "Neuronale Netze sind Mulit-Layer-Perzeptren (MLP), also mehrschichtige Perzeptren, wie in der folgenden Darstellung abgebildt. Damit es nicht so unübersichtlich wird, sind Kreise mit dem Symbol $\\Sigma | |Phi$ so zu verstehen, dass zuerst die gewichtete Summe gebildet wird und dann die Aktivierungsfunktion angewendet wird.\n",
    "\n",
    "![MLP](pics/mlp.png \"MLP\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca24d9f-024b-4a76-89ba-4306fb9ac0f2",
   "metadata": {},
   "source": [
    "Diese MLP können beliebig zusammengestzt werden. In dem oben gezeigten Beispiel haben hat das neuronale Netz eine **Eingabeschicht**, eine sogenannte **verdeckte Schicht**, die Zwischenergebnisse speichert, und eine **Ausgabeschicht**. Fügt man mehere verdeckte Zwischenschichten hinzu, spricht man von tiefen neuronalen Netzen, also von einem **Deep Neural Network**. Das Lernen der Gewichte geht mit steigender Anzahl von Schichten und vor allem auch mit einer größeren Anzahl von Neuronen pro Schicht nicht mehr einfach.   \n",
    "\n",
    "Schauen wir uns an, wie das Training eines tiefen neuronalen Netzes in Scikit-Learn funktioniert. Dazu benutzen wir aus dem Untermodul ``sklearn.neural_network`` den ``MLPClassifier``, also ein Multi-Layer-Perzeptron für Klassifikationsaufgaben:\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n",
    "Wir benutzen künstliche Daten, um die Anwendung des MLPClassifiers zu demonstrieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306e8bc-3de9-47e7-8be0-3223ccaa6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# erzeuge künstliche Daten\n",
    "X,y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], s=20, c=y, cmap='autumn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb46d4c-d2d9-4e00-a9ac-bf3dc76075be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Auswahl des Models\n",
    "# solver = 'lbfgs' für kleine Datenmengen, solver = 'adam' für große Datenmengen, eher ab 10000\n",
    "# alpha = Lernrate, siehe Erklärungen oben\n",
    "# hidden_layer: Anzahl der Neuronen pro verdeckte Schicht und Anzahl der verdeckten Schichten\n",
    "model = MLPClassifier(solver='lbfgs', alpha=0.1, hidden_layer_sizes=[5, 5])\n",
    "\n",
    "# Split Trainings- / Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validierung \n",
    "score_train = model.score(X_train, y_train)\n",
    "score_test = model.score(X_test, y_test)\n",
    "print('Score für Trainingsdaten: {:.2f}'.format(score_train))\n",
    "print('Score für Testdaten: {:.2f}'.format(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a501dc3-df0b-45a2-8f0a-8bc9fa296aee",
   "metadata": {},
   "source": [
    "Wir zeichen die Entscheidungsgrenzen ein, um zu sehen, wo das neuronale Netzt die Trennlinien zieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96af0e9-4735-48c8-9941-f17f1f67fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridX, gridY = np.meshgrid(np.linspace(-1.5, 1.5), np.linspace(-1.5, 1.5))\n",
    "gridZ = model.predict_proba(np.column_stack([gridX.ravel(), gridY.ravel()]))[:, 1]\n",
    "Z = gridZ.reshape(gridX.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], s=20, c=y, cmap='autumn');\n",
    "ax.contourf(gridX, gridY, Z, cmap='autumn', alpha=0.1);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80257b79-e4c1-4fcf-8dfc-2a739f3dd552",
   "metadata": {},
   "source": [
    "Im Folgenden wollen wir uns ansehen, welche Bedeutung die optionalen Parameter haben. Dazu zunächst noch einmal der komplette Code, aber ohne einen Split in Trainings- und Testdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab600cba-9ad3-433b-aed3-a851444d7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setze verschiedene Werte für alpha und Architektur der verdeckten Schicht\n",
    "my_alpha = 0.1\n",
    "my_hidden_layers = [10,10]\n",
    "\n",
    "# erzeuge künstliche Daten\n",
    "X,y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "# Auswahl des Model\n",
    "model = MLPClassifier(solver='lbfgs', alpha=my_alpha, hidden_layer_sizes=my_hidden_layers)\n",
    "\n",
    "# Training und Validierung\n",
    "model.fit(X, y)\n",
    "print('Score: {:.2f}'.format(model.score(X, y)))\n",
    "\n",
    "# Visualisierung\n",
    "gridX, gridY = np.meshgrid(np.linspace(-1.5, 1.5), np.linspace(-1.5, 1.5))\n",
    "gridZ = model.predict_proba(np.column_stack([gridX.ravel(), gridY.ravel()]))[:, 1]\n",
    "Z = gridZ.reshape(gridX.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], s=20, c=y, cmap='autumn');\n",
    "ax.contourf(gridX, gridY, Z, cmap='autumn', alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fcdc7d-1928-4af0-941f-15ed4cd4f9d0",
   "metadata": {},
   "source": [
    "Wie Sie sehen, ist es schwieirg, ein gutes Verhältnis von Lernrate $\\alpha$ und der Architektur des neuronalen Netzes (= Anzahl der Neuronen pro verdeckter Schicht und Anzahl verdeckter Schichten) zu finden. Auch fällt das Ergebnis jedesmal ein wenig anders aus, weil stochastische Verfahren im Hintergrund für das Trainieren der Gewichte benutzt werden. Aus diesem Grund sollten neuronale Netze nur eingesetzt werden, wenn sehr große Datenmengen vorliegen und dann noch ist das Finden der besten Architektur eine große Herausforderung."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

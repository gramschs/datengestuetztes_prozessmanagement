{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109008bb-dc27-451b-a70d-9bb8b0a8f3ba",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines könnten zwar mit Stützvektor-Maschinen übersetzt werden, aber tatsächlich hat sich auch im deutschprachigen Raum der englische Fachbegriff eingebürgert. Allerdings wird meist nur die Abkürzung verwendet, also SVM.  \n",
    "\n",
    "SVMs gehören zum überwachten maschinellem Lernen (Supervised Learning). Sie können sowohl für Klassifikations- also auch Regressionsprobleme eingesetzt werden. Nachdem wir aber uns zu letzt mit linearer und polynomialer Regression beschäftigt haben, führen wir die SVMs anhand von Klassifikationsproblemen ein.\n",
    "\n",
    "Wie üblich laden wir einige Module, die wir brauchen. \n",
    "\n",
    "Hinweis: Die folgenden Code-Beispiels sind größtenteils dem Buch \"Data Science mit Python\"von Jake VanderPlas übernommen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743f820-03d4-4f31-9c74-ab7f755f0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a33d71-0bc5-4dac-b383-73c54aa8ac67",
   "metadata": {},
   "source": [
    "Genau wie in der letzten Woche möchten wir vorerst keine echten Daten importieren, sondern erzeugen uns künstliche Daten. Dazu nehmen wir Scikit-Learn zur Hilfe, das die Funktion ``make_blobs`` zur Verfügung stellt. Details dazu finden Sie in der Dokumentation hier: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74a1eb-6ada-4f64-b655-2c1324db5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c664b-7c3d-46b3-9094-76710f42b47f",
   "metadata": {},
   "source": [
    "Die Messdaten haben zwei Attribute (Inputs) und ein Target (Output), das nur die Zugehörigkeit zu einer Klasse (gelb oder rot) kennzeichnet. Sie können auch in X und y hineinsehen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08c451-8b39-49d9-bfc9-5633a3da51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3592c-156e-4c7b-ad46-7db5c7d068c1",
   "metadata": {},
   "source": [
    "Der Output y enthält entweder eine 0 oder eine 1. In der Grafik steht rot für 0 und gelb für 1.\n",
    "\n",
    "### Ziel der Klassifikation mit SVM-Algorithmen\n",
    "*Ziel: Wir möchten nun eine Gerade (oder später verallgemeinert eine Kurve) finden, die eindeutig die roten Bubbles von den gelben trennt.*\n",
    "\n",
    "Es ist nicht so schwierig, sich lineare Funktionen oder Geraden auszudenken, die die roten und gelben Bubbles trennen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dca9ae-27f5-498a-bfb7-761573685ac7",
   "metadata": {},
   "source": [
    "**Mini-Übung**\n",
    "\n",
    "Denken Sie sich eine Gerade aus, die rote und gelbe Bubbles trennt und zeichnen Sie diese ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8cf52-2ec4-49c6-80dc-d8ac4cd55d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Ihr Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44cffc-241f-4447-a862-8a3a7cef07e6",
   "metadata": {},
   "source": [
    "Wenn wir unsere Geraden vergleichen würden, hat wahrscheinlich jeder von uns eine andere Gerade gewählt. Und damit sind wir wieder bei dem Problem: welches Modell ist das beste?\n",
    "\n",
    "Im Folgenden sehen Sie einige Geraden eingezeichnet. Je nachdem, für welche Gerade wir uns entscheiden würden, würde das blaue Kreuz zu den roten oder zu den gelben Bubbles gehören. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae8e4a-3498-488f-97ee-defcf67c584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='blue', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce3756-809b-4033-88b4-987b6e88770d",
   "metadata": {},
   "source": [
    "### Idee SVM: maximiere den Rand\n",
    "\n",
    "Die Idee der SVM-Algorithmen ist es, nicht nur eine Linie (Kurve) zu finden, die die einzelnen Datenpunkte voneinander trennt, sondern diejenige Linie (Kurve), de auch noch den breitesten Rand hat.\n",
    "\n",
    "Wir zeichnen nun zu den drei Linien Ränder ein. Dabei wählen wir den Rand so breit, bis zum ersten Mal ein Datenpunkt getroffen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed8be2-dfbe-4b18-b0d1-b7dcf493b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    ax.plot(xfit, yfit, '-k')\n",
    "    ax.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "ax.set_xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cb6ed-4541-4d44-bc21-53819c6900a8",
   "metadata": {},
   "source": [
    "Bei einer Klassifikation mit SVM wählen wir nun diejenige Linie, die den maximalen Rand zu den Datenpunkte hat, also die mittlere Linie. Ein SVM ist ein **Breiter-Rand-Klassifikator**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05415430-7919-497f-8647-6b87cc7f7d8b",
   "metadata": {},
   "source": [
    "### SVM mit Scikit-Learn bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a3e66-470c-4b9b-80fc-4022ebdd9a2d",
   "metadata": {},
   "source": [
    "Scikit-Learn enthält sowohl für die Regression als auch für die Klassifikation Support-Vector-Machines-Algorithmen. Das Untermodul für SVMs heißt ``sklearn.svm``. Da wir ein Klassifikationsproblem modellieren wollen, importieren wir einen SVM Classifier, kurz SVC. Die Dokumentation dazu finden Sie hier\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "Danach wählen wir ein Modell aus. Wir entscheiden uns für eine lineare Funktion und setzen daher den ersten Hyperparameter ``kernel`` auf ``'linear'``. Zusätzlich müssen wir noch einen Hyperparameter setzen, der bestimmt, wie strikt wir bei Verletzungen der Ränder sind. Für den Anfang wollen wir den breitesten Rand, der möglich ist, und setzen daher den Hyperparameter ``C`` auf den Wert ``1E10``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52284bb3-2864-4ab5-b9e7-59cb5656e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Auswahl Modell\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "\n",
    "# Training \n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c9d35-8098-4810-83ee-b31a0621f708",
   "metadata": {},
   "source": [
    "Um jetzt besser zu verstehen, was der SVM-Algorithmus gemacht hat, übernehmen wir von Jake VanderPlas eine Funktion, die uns die Ränder einzeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ee6db-149a-4fe2-bf67-b5a909a4270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    decision_function = model.decision_function(xy)\n",
    "    if decision_function.ndim == 1:\n",
    "        P = decision_function.reshape(X.shape)\n",
    "        ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5,linestyles=['--', '-', '--'])\n",
    "    else:\n",
    "        for i in range(np.shape(decision_function)[1]):\n",
    "            P = decision_function[:,i].reshape(X.shape)\n",
    "            # plot decision boundary and margins\n",
    "            ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5,linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none', edgecolors='black');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd473ff-9d7e-4cbf-9b40-ae9786019121",
   "metadata": {},
   "source": [
    "Mit dieser Funktion zeichen wir nun das Ergebnis des trainierten Modells zusammen mit den Rändern ein. Zusätzlich werden einige Punkte eingekreist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44a176-5c47-472e-8f59-4c9cc380a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f68d6d-adf1-4e88-8ee8-929e51e609a9",
   "metadata": {},
   "source": [
    "Es gibt zwei rote Punkte und einen gelben Punkt, die direkt den Rand berühren. Die Ortsvektoren, die vom Koordinatenursprung zu diesen Punkten zeigen, heißen Stützvektoren, also auf Englisch **support vector** und haben dieser Methode zu ihrem Namen verholfen. \n",
    "\n",
    "Weil diese Support Vectors wichtig sind, werden Sie nach dem Training im Modell abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb8feb-b8b7-4865-9db0-8cd5cb2c8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec0fe2-f06b-41db-a88b-d346794c0ea2",
   "metadata": {},
   "source": [
    "Support Vector Machines sind sehr beliebt. Einer der wichtigsten Gründe für ihre Beliebtheit ist, dass nur noch die Position der Stützvektoren entscheidet, wo die Datnepunkte getrennt werden . Das ist eine simple und vor allem auch leicht nachvollziehbare Entscheidung des Algorithmus. Bei vielen anderen ML-Algorithmen erkennt man nur schwer, was dazu geführt hat, dass dieses Modellergebnis herauskommt.\n",
    "\n",
    "Ein zweiter Grund ist, dass jeder zusätzliche Punkt, der dazu kommt, die Stützvektoren nicht ändert, sofern der Punkt auf der richtigen Seite liegt. Damit lohnt es sich, SVMs auch mit Daten zu trainieren, wenn man weiß, dass später neue Messungen hinzukommen.\n",
    "\n",
    "Hier ein Beispiel, bei dem wir uns ansehen, was das Modell durch die ersten 60 Punkte und was durch die ersten 120 Punkte gelernt hat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b336cb1-495c-4edc-8744-7dab12d184ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f3fea-b000-45ac-a830-1ac6024d0e4a",
   "metadata": {},
   "source": [
    "Wir sehen, dass bereits die ersten 60 Punkte (linkes Bild) zu einem sehr guten Modell geführt haben. Acu wenn wir weitere 60 Punkte hinzunehmen (rechtes Bild zeigt die ersten 120 Trainingsdaten), bleibt die Gerade und auch dieselben Stützvektoren werden gefunden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4dd80-ae88-40f4-87a3-fc43a77d997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipywidgets import interact, fixed\n",
    "#interact(plot_svm, N=np.arange(10,200,20), ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada030f-61d0-464f-92fe-d53e7988efd1",
   "metadata": {},
   "source": [
    "### SVM mit Kernel-Methoden kombiniert\n",
    "\n",
    "Für die polynomiale Regression hat Sckit-Learn gar keine eigenständige Funktion. Stattdessen haben wir auf einen Trick zurückgegriffen, bei dem aus einem niedrig-dimensionalen Raum (wenige Eigenschaften, Attribute, Features) ein höher-dimensionaler Raum gemacht wurde: Stichwort Kernel-Trick! Auch hier wollen wir dieses Konzept nutzen.\n",
    "\n",
    "Zuerst generieren wir uns wieder künstliche Daten. Scikit-Learn stellt uns eine Funktion zur Verfügung, mit der wir Trainingsdaten in Kreisen erzeugen können, nämlich ``make_circles`` aus dem Untermodul ``sklearn.datasets``. Weitere Details finden Sie in der Dokumentation: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html?highlight=make%20circles#sklearn.datasets.make_circles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5f93a-672e-4de6-b90b-41c9ba895257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Erzeugung künstlicher Trainingsdaten\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda53ae-d494-4606-b672-0006b91a41da",
   "metadata": {},
   "source": [
    "Keine einzige Gerade, die wir uns ausdenken, trennt die gelben von den roten Bubbles. Aber wir können eine Dimension, eine weitere Eigenschaft hinzunehmen. Die Position der Daten wird durch ihre x-Koordinate und ihre y-Koordinate beschrieben. Wir wollen jetzt noch eine z-Koordinate in die Höhe hinzufügen. Dabei soll die z-Koordinate umso höher sein, je n|aher der Datenpunkt am Ursprung (0,0) liegt. \n",
    "\n",
    "Bevor wir die Messdaten in eine höhere Dimension projizieren, basteln wir uns erstmal eine Funktion, die prinzipiell diese Gestalt hat. Im eindimensionalen gedacht soll an der Stelle 0 der Wert 1 (entspricht 100 %) erzielt werden und danach soll die Funktion schnell abfallen. Das erreichen wir beispielsweise mit einer Exponentialfunktion mit einem negativen Exponenten (Hochzahl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16193578-74b7-4e1d-8e84-6724c942e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5)\n",
    "y = np.exp(-x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86296de-4501-4739-bc32-5d3201eb00cd",
   "metadata": {},
   "source": [
    "Jetzt brauchen wir aber eine Funktion, die von zwei Koordinaten abhängt, denn wir haben ja zwei Input-Features. Wir berechnen zu jeden Punkt den Abstand zum Ursprung (Wurzelziehen schenken wir uns). Dann sieht die Funktion folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a59990-54dc-4701-8cc3-2ade0412af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Gitter erzeugen\n",
    "X_grid, Y_grid = np.meshgrid(np.linspace(-3, 3), np.linspace(-3, 3))\n",
    "\n",
    "# Funktion auswerten und Höheninformationen berechnen\n",
    "z = np.exp( -(X_grid**2 + Y_grid**2) )\n",
    "           \n",
    "# 3D Plot \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X_grid, Y_grid, z, cmap='viridis');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56a331-35fa-4960-903c-338a799ab5f6",
   "metadata": {},
   "source": [
    "Jetzt nutzen wir genau diese Funktion, um die z-Koordinate für unsere Trainingsdaten zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58ba31-c793-498f-9ba2-ea175ec5a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung künstlicher Trainingsdaten\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# Kernel-Trick\n",
    "z = np.exp( -(X[:,0]**2 + X[:,1]**2) )\n",
    "\n",
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], z, s=30, c=y, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776d848-67d3-4b5a-bf76-56fc0f856bf5",
   "metadata": {},
   "source": [
    "Wenn wir uns jetzt die 3D-Daten ansehen, können wir sehr wohl die gelben von den roten Bubbles unterscheiden: eine Ebene mit der Ebenengleichung $z = 0.7$ würde beispielsweise funktionieren.\n",
    "\n",
    "Die vorgestellte Vorgehensweise hat einen Nachteil. Unsere Messdaten wurden künstlich erzeugt, indem zwei Kreise um den Ursprung erzeugt aurden und dann die Bublles ein wenig verrauscht und gestört wurden. Daher fiel es uns einfach zu raten, dass die Kernel-Methode mit einer Exponentialfunktion bezogen auf den Abstand der Punkte zum Ursprung gut funktionieren würde. Aber wie finden wir für nicht künstlich erzeugte Messdaten eine passende Kernel-Funktion? \n",
    "\n",
    "Tatsächlich wird in der Praxis für jeden einzelnen Datenpunkt eine eigene Kernel-Funktion genutzt, mit der in den höherdimensionalen Raum projiziert wird. Wenn wir dabei ganz bestimmte Kernel-Funktionen nehmen, ist die Berechnung auch nicht allzu aufwendig, weil wir Funktionseigenschaften geschickt ausnutzen können. \n",
    "\n",
    "### Finetuning des 1. Hyperparameters: 'linear' oder 'rbf'\n",
    "\n",
    "In Scikit-Learn können wir die dort implementierten Algorithmen einfach nutzen, in dem wir den Hyperparameter ``'linear'`` auf ``'rbf'`` umstellen. Dabei steht rbf für radiale Basis-Funktion und meint die mehrdimensionalen Exponentialfunktionen, die vom Abstand abhängen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029065fc-2900-4d63-a772-b6937e943b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung künstlicher Trainingsdaten\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# Auswahl Modell\n",
    "model = SVC(kernel='rbf', C=1E10)\n",
    "\n",
    "# Training \n",
    "model.fit(X, y)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d33d2e-525f-44f4-870a-c198c0b4aa27",
   "metadata": {},
   "source": [
    "### Finetuning des 2. Hyperparameters: C\n",
    "\n",
    "Bei den künstlich erzeugten Messdaten haben wir zwei verrauschte Kreise gewählt, die zwar nicht durch eine Gerade getrennt werden konnten, aber eindeutig durch eine Kurve. Was aber, wenn die Messdaten ein wenig überlappen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aed861-78a6-44e8-9b18-6f101eb86c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Daten, die überlappen\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeeaa81-31a4-4c85-ac0a-a6db14b884f0",
   "metadata": {},
   "source": [
    "Für diesen Fall gibt es den 2. Hyperparameter, das ``'C'``, mit dem die Härte des Randes eingestellt wird. Eine große Zahl bedeutet, dass die Grenze absolt hart und unüberwindlich ist. Am besten sehen wir uns das für unsere Trainingsdaten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a70e8-e41b-42e9-86df-df5d6582326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Messdaten\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)\n",
    "\n",
    "# zeichne Messdaten\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# trainiere zwei SVM: C = 10 und C = 0.1 und zeichne die Ränder \n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9776084-81f6-436a-b4f4-9914aad566da",
   "metadata": {},
   "source": [
    "Bei C = 10 ragt kein Messpunkt in den Randbereich. Aufgrund der Positionen der Messpunkte ist dafür der Randbereich sehr klein. Die rechte Grafik zeigt, dass einige rote und gelbe Bubbles im Randbereich liegen. \n",
    "\n",
    "Natürlich stellt sich als nächstes die Frage: welches C sollen wir wählen? Diese Frage wird wie zuvor bei der (linearen und polynomialen) Regression dadurch beantwortet, dass man die Daten in Trainings- und Testdaten aufteilt und die Prognosequalität misst. Einen R2-Score gibt es nur bei Regressionspolynomen. Für die Support Vector Machines wird einfach nur der Score berechnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46473a-c721-4d61-9ddc-94e2fe104372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# erzeuge künstliche Messdaten\n",
    "X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.8)\n",
    "\n",
    "# teile in Trainings- und Testdaten auf\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "score_training = []\n",
    "score_testing = []\n",
    "\n",
    "# FOR-Schleife\n",
    "for C in [0.1, 1, 10, 100, 1000, 1e10]:\n",
    "    # Auswahl des Modells \n",
    "    model = SVC(kernel='linear', C=C)\n",
    "\n",
    "    # Training \n",
    "    model.fit(X_train, y_train)\n",
    "   \n",
    "    # Validierung\n",
    "    sc_train = model.score(X_train, y_train)\n",
    "    sc_test  = model.score(X_test, y_test)\n",
    "   \n",
    "    score_training.append(sc_train)\n",
    "    score_testing.append(sc_test)\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model)\n",
    "    ax.set_title('C = {}'.format(C))\n",
    "\n",
    "print(score_training)\n",
    "print(score_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c4d1e-58ce-4015-bd27-78da7377979a",
   "metadata": {},
   "source": [
    "# Übungsaufgaben zur Vertiefung\n",
    "\n",
    "#### Aufgabe 12.1\n",
    "\n",
    "Laden Sie den Datensatz mit Brustkrebsdaten, der in Scikit-Learn hinterlegt ist: \n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html \n",
    "\n",
    "Tipp: funktioniert analog zu der Iris-Datensammlung wie in Part 10.\n",
    "\n",
    "Führen Sie eine Klassifikation mit einer **linearen SVM** durch, um aus den ersten beiden Features ('mean radius' und 'mean texture') prognostizieren zu können, ob der Tumor bösartig (0) oder gutartig (1) ist. Splitten Sie dazu die Daten in Trainings- und Testdaten, berechnen Sie den Score des Modells und visualisieren Sie am Ende Ihr Ergebnis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd723f8-f7ca-4272-b420-7bbd66d89813",
   "metadata": {},
   "source": [
    "#### Aufgabe 12.2\n",
    "\n",
    "Laden Sie den Datensatz mit Brustkrebsdaten, der in Scikit-Learn hinterlegt ist: \n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html \n",
    "\n",
    "Tipp: funktioniert analog zu der Iris-Datensammlung wie in Part 10.\n",
    "\n",
    "Führen Sie eine Klassifikation mit einer **Kernel-SVM** (radiale Basis-Funktion) durch, um aus den ersten beiden Features ('mean radius' und 'mean texture') prognostizieren zu können, ob der Tumor bösartig (0) oder gutartig (1) ist. Splitten Sie dazu die Daten in Trainings- und Testdaten, berechnen Sie den Score des Modells und visualisieren Sie am Ende Ihr Ergebnis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ff9fd-8100-4aed-9aff-348a0de7d295",
   "metadata": {},
   "source": [
    "#### Aufgabe 12.3 Wiederholung\n",
    "\n",
    "Wiederholen Sie die Themen Numpy und Pandas (Part 5 und 6).\n",
    "\n",
    "* Welchen Funktionalitäten bieten die beiden Module?\n",
    "* Wie viele Dimensionen kann ein Numpy-Array haben?\n",
    "* Erzeugen Sie ein 1d-Array mit allen geraden Zahlen zwischen 0 und 20.\n",
    "* Erzeugen Sie ein 3x5-Matrix (2d-Array) gefüllt mit 0.\n",
    "* Erzeugen Sie ein 3x5-Array mit zufälligen Zahlen zwischen 0 und 100 und summieren Sie entlang der Zeilen.\n",
    "* Was ist der Unterschied zwischen einem Series-Objekt und einem DataFrame-Objekt?\n",
    "* Angenommen, Sie haben einen DataFrame mit den Wochentagen Montag, Dienstag, ... als Zeilenindex und den Namen Alics, Bob und Charlie als Spaltenindex. Die Daten sind die Anzahl der Wochenstunden Vorlesung der Person an dem Tag. \n",
    "    * Wie greifen Sie auf alle Daten von Bob zu?\n",
    "    * Wie greifen Sie auf den Inhalt der Zelle von Alice am Mittwoch zu? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
